{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bc0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Active state example NCAA hoops predictor script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2200aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# you can also use = glob.glob('../\n",
    "#these files are from (https://www.kaggle.com/c/mens-march-mania-2022/data)\n",
    "\n",
    "tcr_df = pd.read_csv('../google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/Basics/MNCAATourneyCompactResults.csv')\n",
    "ts_df = pd.read_csv('../google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/Basics/MNCAATourneySeeds.csv')\n",
    "rscr_df = pd.read_csv('../google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/Basics/MRegularSeasonCompactResults.csv')\n",
    "ss_df = pd.read_csv('../google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/Basics/MSampleSubmissionStage1_2020.csv')\n",
    "massey_df = pd.read_csv('../google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/PublicRankings/MMasseyOrdinals.csv')\n",
    "tbs_df = pd.read_csv('../google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/TeamBoxScores/MNCAATourneyDetailedResults.csv')\n",
    "rsbs_df = pd.read_csv('../google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/TeamBoxScores/MRegularSeasonDetailedResults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e028a",
   "metadata": {},
   "source": [
    "# First we normalized the number of points scored in a game to account for games that go into overtime (otherwise, the points-per-game statistic is overestimated for the teams that play more overtimes). Then we averaged the points-per-game across every game in each season. We also calculated the number of games in each season and win percentage for the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc11e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppg(rscr_df):\n",
    "    \n",
    "    #Gametime normalization \n",
    "    rscr_df['GameDuration'] = rscr_df['NumOT']\n",
    "    for i in range(len(rscr_df.NumOT.value_counts())):\n",
    "        rscr_df.loc[rscr_df['NumOT'] == rscr_df.NumOT.value_counts().index[i], 'GameDuration'] = 40 + i*5\n",
    "    rscr_df['Wppg'] = (rscr_df['WScore'] / rscr_df['GameDuration']) * 40\n",
    "    rscr_df['Lppg'] = (rscr_df['LScore'] / rscr_df['GameDuration']) * 40\n",
    "    \n",
    "    #Drop unnecesary columns \n",
    "    rscr_df = rscr_df.drop(['DayNum', 'WLoc', 'NumOT', 'WScore','LScore','GameDuration'], axis = 1)\n",
    "    \n",
    "    #Calculate average points per game and number of games for winning and losing team\n",
    "    Wppg_df = rscr_df[[\"Season\",\"WTeamID\",'Wppg']].groupby([\"Season\",\"WTeamID\"]).agg(['mean', 'count'])\n",
    "    Lppg_df = rscr_df[[\"Season\",\"LTeamID\",'Lppg']].groupby([\"Season\",\"LTeamID\"]).agg(['mean', 'count'])\n",
    "    Wppg_df.columns = Wppg_df.columns.droplevel(0)\n",
    "    Wppg_df = Wppg_df.reset_index().rename(columns = {'mean':'Wppg', 'count':'WGames'})\n",
    "    Lppg_df.columns = Lppg_df.columns.droplevel(0)\n",
    "    Lppg_df = Lppg_df.reset_index().rename(columns = {'mean':'Lppg', 'count':'LGames'})\n",
    "    \n",
    "    #Merge PPG for wins and losses and fill NaNs with 0s for undefeated and no-win teams\n",
    "    ppg_df = pd.merge(Wppg_df, Lppg_df, left_on=['Season', 'WTeamID'], right_on=['Season', 'LTeamID'], how='outer')\n",
    "    ppg_df = ppg_df.fillna({'WTeamID': ppg_df.LTeamID, 'Wppg': 0, 'WGames': 0, 'LTeamID': ppg_df.WTeamID , 'Lppg': 0, 'LGames': 0})\n",
    "    \n",
    "    #Calculate PPG and win percentage for season \n",
    "    ppg_df['PPG'] = (ppg_df['Wppg']*ppg_df['WGames'] + ppg_df['Lppg']*ppg_df['LGames'])/(ppg_df['WGames'] + ppg_df['LGames'])\n",
    "    ppg_df['WPerc'] = (ppg_df['WGames'])/(ppg_df['WGames'] + ppg_df['LGames'])\n",
    "    ppg_df['TeamID'] = ppg_df['WTeamID'].astype('int32')\n",
    "    ppg_df['GamesPlayed'] = (ppg_df['WGames'] + ppg_df['LGames'])\n",
    "    ppg_df = ppg_df.drop(['WTeamID', 'Wppg' ,'WGames','LTeamID', 'Lppg','LGames'],axis = 1)\n",
    "    \n",
    "    return ppg_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b2f556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>DayNum</th>\n",
       "      <th>WTeamID</th>\n",
       "      <th>WScore</th>\n",
       "      <th>LTeamID</th>\n",
       "      <th>LScore</th>\n",
       "      <th>WLoc</th>\n",
       "      <th>NumOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985</td>\n",
       "      <td>20</td>\n",
       "      <td>1228</td>\n",
       "      <td>81</td>\n",
       "      <td>1328</td>\n",
       "      <td>64</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1985</td>\n",
       "      <td>25</td>\n",
       "      <td>1106</td>\n",
       "      <td>77</td>\n",
       "      <td>1354</td>\n",
       "      <td>70</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1985</td>\n",
       "      <td>25</td>\n",
       "      <td>1112</td>\n",
       "      <td>63</td>\n",
       "      <td>1223</td>\n",
       "      <td>56</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985</td>\n",
       "      <td>25</td>\n",
       "      <td>1165</td>\n",
       "      <td>70</td>\n",
       "      <td>1432</td>\n",
       "      <td>54</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985</td>\n",
       "      <td>25</td>\n",
       "      <td>1192</td>\n",
       "      <td>86</td>\n",
       "      <td>1447</td>\n",
       "      <td>74</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161547</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1153</td>\n",
       "      <td>69</td>\n",
       "      <td>1222</td>\n",
       "      <td>57</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161548</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1209</td>\n",
       "      <td>73</td>\n",
       "      <td>1426</td>\n",
       "      <td>64</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161549</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1277</td>\n",
       "      <td>65</td>\n",
       "      <td>1276</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161550</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1387</td>\n",
       "      <td>55</td>\n",
       "      <td>1382</td>\n",
       "      <td>53</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161551</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1463</td>\n",
       "      <td>97</td>\n",
       "      <td>1217</td>\n",
       "      <td>85</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161552 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT\n",
       "0         1985      20     1228      81     1328      64    N      0\n",
       "1         1985      25     1106      77     1354      70    H      0\n",
       "2         1985      25     1112      63     1223      56    H      0\n",
       "3         1985      25     1165      70     1432      54    H      0\n",
       "4         1985      25     1192      86     1447      74    H      0\n",
       "...        ...     ...      ...     ...      ...     ...  ...    ...\n",
       "161547    2019     132     1153      69     1222      57    N      0\n",
       "161548    2019     132     1209      73     1426      64    N      0\n",
       "161549    2019     132     1277      65     1276      60    N      0\n",
       "161550    2019     132     1387      55     1382      53    N      0\n",
       "161551    2019     132     1463      97     1217      85    H      0\n",
       "\n",
       "[161552 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149f4a1",
   "metadata": {},
   "source": [
    "# In the function below, I created an offensive metric based on points made, assists, and offensive rebounds, as well as a defensive metric based on opposing team points missed, steals, defensive rebounds, turnovers forced, blocks, and personal fouls. I averaged the metrics across the entire regular season, and also the last 30 days of the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93806823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficiency(rsbs_df):\n",
    "    #Gametime normalization \n",
    "    rsbs_df['GameDuration'] = rsbs_df['NumOT']\n",
    "    for i in range(len(rsbs_df.NumOT.value_counts())):\n",
    "        rsbs_df.loc[rsbs_df['NumOT'] == rsbs_df.NumOT.value_counts().index[i], 'GameDuration'] = 40 + i*5\n",
    "\n",
    "    rsbs_df['WFGM'] = (rsbs_df['WFGM'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WFGA'] = (rsbs_df['WFGA'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WFGM3'] = (rsbs_df['WFGM3'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WFGA3'] = (rsbs_df['WFGA3'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WFTM'] = (rsbs_df['WFTM'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WFTA'] = (rsbs_df['WFTA'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WOR'] = (rsbs_df['WOR'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WDR'] = (rsbs_df['WDR'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WAst'] = (rsbs_df['WAst'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WTO'] = (rsbs_df['WTO'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WStl'] = (rsbs_df['WStl'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WBlk'] = (rsbs_df['WBlk'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['WPF'] = (rsbs_df['WPF'] / rsbs_df['GameDuration']) * 40\n",
    "\n",
    "    rsbs_df['LFGM'] = (rsbs_df['LFGM'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LFGA'] = (rsbs_df['LFGA'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LFGM3'] = (rsbs_df['LFGM3'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LFGA3'] = (rsbs_df['LFGA3'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LFTM'] = (rsbs_df['LFTM'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LFTA'] = (rsbs_df['LFTA'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LOR'] = (rsbs_df['LOR'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LDR'] = (rsbs_df['LDR'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LAst'] = (rsbs_df['LAst'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LTO'] = (rsbs_df['LTO'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LStl'] = (rsbs_df['LStl'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LBlk'] = (rsbs_df['LBlk'] / rsbs_df['GameDuration']) * 40\n",
    "    rsbs_df['LPF'] = (rsbs_df['LPF'] / rsbs_df['GameDuration']) * 40\n",
    "    \n",
    "    #Calculate total points made and missed per game \n",
    "    rsbs_df['wPointsMade'] = (2* (rsbs_df['WFGM'] - rsbs_df['WFGM3'])) + 3*rsbs_df['WFGM3'] + rsbs_df['WFTM']\n",
    "    rsbs_df['wPointsMissed'] = (2* (rsbs_df['WFGA'] - rsbs_df['WFGA3'])) + 3*rsbs_df['WFGA3'] + rsbs_df['WFTA'] - rsbs_df['wPointsMade']\n",
    "    rsbs_df['lPointsMade'] = (2* (rsbs_df['LFGM'] - rsbs_df['LFGM3'])) + 3*rsbs_df['LFGM3'] + rsbs_df['LFTM']\n",
    "    rsbs_df['lPointsMissed'] = (2* (rsbs_df['LFGA'] - rsbs_df['LFGA3'])) + 3*rsbs_df['LFGA3'] + rsbs_df['LFTA'] - rsbs_df['lPointsMade']\n",
    "    \n",
    "    #Calculate offensive and defensive efficiency metrics for winning and losing team \n",
    "    rsbs_df['WoEFF'] = rsbs_df['wPointsMade'] +  rsbs_df['WAst'] + rsbs_df['WOR']\n",
    "    rsbs_df['WdEFF'] = rsbs_df['lPointsMissed'] + rsbs_df['WStl'] + rsbs_df['WBlk'] + rsbs_df['WDR'] + rsbs_df['LTO'] - rsbs_df['WPF']\n",
    "    rsbs_df['LoEFF'] = rsbs_df['lPointsMade'] +  rsbs_df['LAst'] + rsbs_df['LOR']\n",
    "    rsbs_df['LdEFF'] = rsbs_df['wPointsMissed'] + rsbs_df['LStl'] + rsbs_df['LBlk'] + rsbs_df['LDR'] + rsbs_df['WTO'] - rsbs_df['LPF']\n",
    "    \n",
    "    #Extract relevant columns and rename \n",
    "    Weff_df = rsbs_df[['Season', 'DayNum', 'WTeamID','WoEFF', 'WdEFF']]\n",
    "    Weff_df = Weff_df.rename(columns = {'WTeamID': 'TeamID','WoEFF': 'oEFF', 'WdEFF': 'dEFF'})\n",
    "    Leff_df = rsbs_df[['Season', 'DayNum', 'LTeamID','LoEFF', 'LdEFF']]\n",
    "    Leff_df = Leff_df.rename(columns = {'LTeamID': 'TeamID','LoEFF': 'oEFF', 'LdEFF': 'dEFF'})\n",
    "    eff_df = pd.concat([Weff_df, Leff_df]) \n",
    "    \n",
    "    #Take seasonal and 30 day averages of the efficiency metrics\n",
    "    # the 30-day efficiency metrics more accurately capture how â€˜hotâ€™ a team is, going into the tournament.\n",
    "    \n",
    "    effseas_df = eff_df.groupby(by = ['Season', 'TeamID']).agg('mean').reset_index().drop(['DayNum'], axis = 1)\n",
    "    eff30day_df = eff_df[eff_df.DayNum >= 100].groupby(by = ['Season', 'TeamID']).agg('mean').reset_index().drop(['DayNum'], axis = 1)\n",
    "    eff30day_df = eff30day_df.rename(columns = {'oEFF': 'oEFF_30day', 'dEFF': 'dEFF_30day' })\n",
    "    eff_df = pd.merge(effseas_df,eff30day_df, how = 'outer', on = ['Season', 'TeamID']) \n",
    "    return eff_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095136c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>DayNum</th>\n",
       "      <th>WTeamID</th>\n",
       "      <th>WScore</th>\n",
       "      <th>LTeamID</th>\n",
       "      <th>LScore</th>\n",
       "      <th>WLoc</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>WFGM</th>\n",
       "      <th>WFGA</th>\n",
       "      <th>...</th>\n",
       "      <th>LFGA3</th>\n",
       "      <th>LFTM</th>\n",
       "      <th>LFTA</th>\n",
       "      <th>LOR</th>\n",
       "      <th>LDR</th>\n",
       "      <th>LAst</th>\n",
       "      <th>LTO</th>\n",
       "      <th>LStl</th>\n",
       "      <th>LBlk</th>\n",
       "      <th>LPF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>10</td>\n",
       "      <td>1104</td>\n",
       "      <td>68</td>\n",
       "      <td>1328</td>\n",
       "      <td>62</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>10</td>\n",
       "      <td>1272</td>\n",
       "      <td>70</td>\n",
       "      <td>1393</td>\n",
       "      <td>63</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>11</td>\n",
       "      <td>1266</td>\n",
       "      <td>73</td>\n",
       "      <td>1437</td>\n",
       "      <td>61</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>11</td>\n",
       "      <td>1296</td>\n",
       "      <td>56</td>\n",
       "      <td>1457</td>\n",
       "      <td>50</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>11</td>\n",
       "      <td>1400</td>\n",
       "      <td>77</td>\n",
       "      <td>1208</td>\n",
       "      <td>71</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87499</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1153</td>\n",
       "      <td>69</td>\n",
       "      <td>1222</td>\n",
       "      <td>57</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87500</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1209</td>\n",
       "      <td>73</td>\n",
       "      <td>1426</td>\n",
       "      <td>64</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87501</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1277</td>\n",
       "      <td>65</td>\n",
       "      <td>1276</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87502</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1387</td>\n",
       "      <td>55</td>\n",
       "      <td>1382</td>\n",
       "      <td>53</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87503</th>\n",
       "      <td>2019</td>\n",
       "      <td>132</td>\n",
       "      <td>1463</td>\n",
       "      <td>97</td>\n",
       "      <td>1217</td>\n",
       "      <td>85</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87504 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT  WFGM  \\\n",
       "0        2003      10     1104      68     1328      62    N      0    27   \n",
       "1        2003      10     1272      70     1393      63    N      0    26   \n",
       "2        2003      11     1266      73     1437      61    N      0    24   \n",
       "3        2003      11     1296      56     1457      50    N      0    18   \n",
       "4        2003      11     1400      77     1208      71    N      0    30   \n",
       "...       ...     ...      ...     ...      ...     ...  ...    ...   ...   \n",
       "87499    2019     132     1153      69     1222      57    N      0    22   \n",
       "87500    2019     132     1209      73     1426      64    N      0    20   \n",
       "87501    2019     132     1277      65     1276      60    N      0    22   \n",
       "87502    2019     132     1387      55     1382      53    N      0    22   \n",
       "87503    2019     132     1463      97     1217      85    H      0    32   \n",
       "\n",
       "       WFGA  ...  LFGA3  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n",
       "0        58  ...     10    16    22   10   22     8   18     9     2   20  \n",
       "1        62  ...     24     9    20   20   25     7   12     8     6   16  \n",
       "2        58  ...     26    14    23   31   22     9   12     2     5   23  \n",
       "3        38  ...     22     8    15   17   20     9   19     4     3   23  \n",
       "4        61  ...     16    17    27   21   15    12   10     7     1   14  \n",
       "...     ...  ...    ...   ...   ...  ...  ...   ...  ...   ...   ...  ...  \n",
       "87499    50  ...     33    11    18   17   16     8    7     2     4   19  \n",
       "87500    50  ...     33    11    17   13   28    12   14     5     2   24  \n",
       "87501    55  ...     25    10    12    3   26    17    6     5     5   11  \n",
       "87502    59  ...     19     8    10   13   30     9   11     2     7   16  \n",
       "87503    53  ...     32    19    24   12   15     7    9     1     2   22  \n",
       "\n",
       "[87504 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsbs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc84571",
   "metadata": {},
   "source": [
    "# Because weâ€™re interested in the ranking right before the tournament would have begun, we only extracted rankings with the RankingDayNum column equal to 133 (corresponding to the last day of the regular season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c4a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(massey_df):\n",
    "    rankings_df = massey_df[massey_df['RankingDayNum'] == 133]\n",
    "    rankings_df = rankings_df.reset_index().drop(['index','RankingDayNum'], axis = 1)\n",
    "    median_df = rankings_df.groupby(by = ['Season','TeamID'])[['OrdinalRank']].median().reset_index()\n",
    "    median_df = median_df.rename(columns = {'OrdinalRank':'MedianRank'})\n",
    "    mean_df = rankings_df.groupby(by = ['Season','TeamID'])[['OrdinalRank']].mean().reset_index()\n",
    "    mean_df = mean_df.rename(columns = {'OrdinalRank':'MeanRank'})\n",
    "    massey_df = rankings_df[rankings_df['SystemName'] == 'MAS']\n",
    "    massey_df = massey_df.reset_index().drop(['index','SystemName'], axis = 1)\n",
    "    massey_df = massey_df.rename(columns = {'OrdinalRank':'MasseyRank'})\n",
    "    rankings_df = pd.merge(median_df, massey_df, left_on=['Season', 'TeamID'], right_on=['Season', 'TeamID'], how='left')\n",
    "    rankings_df = pd.merge(rankings_df, mean_df, left_on=['Season', 'TeamID'], right_on=['Season', 'TeamID'], how='left')\n",
    "    return rankings_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab5f74",
   "metadata": {},
   "source": [
    "# Before doing that, we should construct a dataframe with the correct submission structure to merge our features onto. Thankfully, Kaggle provides a sample submission file that we can use as a template. The first few rows of the sample look like the following:\n",
    "\n",
    "\n",
    "Where each row represents a matchup in the tournament for each season. The ID consists of the season, the lower Team ID, and the higher Team ID. The Pred column corresponds to the predicted probability of the lower Team ID beating the higher Team ID. Within the context of our model, we used the features we engineered to predict this column. Then, we compared the prediction to the actual result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32cc84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_submission(ss_df,tcr_df):\n",
    "    features_df = ss_df.copy()\n",
    "    \n",
    "    #extract season and team IDs\n",
    "    features_df['Season'] = features_df['ID'].map(lambda x: int(x[:4]))\n",
    "    features_df['WTeamID'] = features_df['ID'].map(lambda x: int(x[5:9]))\n",
    "    features_df['LTeamID'] = features_df['ID'].map(lambda x: int(x[10:14]))\n",
    "    features_df = features_df.rename(columns= {'WTeamID':'TeamID_1','LTeamID':'TeamID_2'})\n",
    "    \n",
    "    #Extract seasons starting from 2015 and create Result column corresponding to winner and loser\n",
    "    Wtcr_df = tcr_df[tcr_df.Season >= 2015].loc[:,['Season', 'WTeamID', 'LTeamID']].reset_index().drop('index', axis = 1)\n",
    "    Wtcr_df = Wtcr_df.rename(columns = {'WTeamID': 'TeamID_1', 'LTeamID': 'TeamID_2'})\n",
    "    Wtcr_df['Result'] = 1\n",
    "    Ltcr_df = tcr_df[tcr_df.Season >= 2015].loc[:,['Season', 'WTeamID', 'LTeamID']].reset_index().drop('index', axis = 1)\n",
    "    Ltcr_df = Ltcr_df.rename(columns = {'WTeamID': 'TeamID_2', 'LTeamID': 'TeamID_1'})\n",
    "    Ltcr_df['Result'] = 0\n",
    "    tcr_df = pd.concat([Wtcr_df,Ltcr_df], sort=False).reset_index().drop('index', axis = 1)\n",
    "    tcr_df['ID'] = tcr_df['Season'].apply(str) + '_' + tcr_df['TeamID_1'].apply(str) + '_' + tcr_df['TeamID_2'].apply(str)\n",
    "    tcr_df = tcr_df.drop(['Season', 'TeamID_1', 'TeamID_2'],axis = 1)\n",
    "    \n",
    "    #merge results onto sample submission\n",
    "    features_df = pd.merge(features_df,tcr_df, how = 'left', on = 'ID')\n",
    "    features_df = features_df.drop(['Pred'],axis = 1)\n",
    "    return features_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9214db5",
   "metadata": {},
   "source": [
    "# Besides generating a sample submission file, the above function also creates our test set, which corresponds to all possible tournament matchups for each of the past seasonâ€™s tournaments. The output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cd52e",
   "metadata": {},
   "source": [
    "# We can create our training set (regular season games) in much the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a12dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_regseason(rscr_df):\n",
    "    \n",
    "    #Extract seasons starting from 2015 and create Result column corresponding to winner and loser\n",
    "    \n",
    "    Wrscr_df = rscr_df[rscr_df.Season >= 2015].loc[:,['Season', 'WTeamID', 'LTeamID']].reset_index().drop('index', axis = 1)\n",
    "    Wrscr_df = Wrscr_df.rename(columns = {'WTeamID': 'TeamID_1', 'LTeamID': 'TeamID_2'})\n",
    "    Wrscr_df['Result'] = 1\n",
    "    Lrscr_df = rscr_df[rscr_df.Season >= 2015].loc[:,['Season', 'WTeamID', 'LTeamID']].reset_index().drop('index', axis = 1)\n",
    "    Lrscr_df = Lrscr_df.rename(columns = {'WTeamID': 'TeamID_2', 'LTeamID': 'TeamID_1'})\n",
    "    Lrscr_df['Result'] = 0\n",
    "    rscr_df = pd.concat([Wrscr_df,Lrscr_df], sort=False).reset_index().drop('index', axis = 1)\n",
    "    rscr_df['ID'] = rscr_df['Season'].apply(str) + '_' + rscr_df['TeamID_1'].apply(str) + '_' + rscr_df['TeamID_2'].apply(str)\n",
    "    return rscr_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01615ad",
   "metadata": {},
   "source": [
    "# Next we got on with merging the engineered features onto both the test and training sets. The function works equally well for both the training and test sets, with the only difference being the first argument of the function:\n",
    "\n",
    "For the test set we used the output of the prep_submission() function.\n",
    "For the training set we used the output of the prep_regseason() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8c7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_features(features_df, ppg_df,eff_df,rankings_df):\n",
    "    \n",
    "    Wppg_df = ppg_df.rename(columns = {'PPG':'PPG_1','WPerc':'WPerc_1'})\n",
    "    Lppg_df = ppg_df.rename(columns = {'PPG':'PPG_2','WPerc':'WPerc_2'})\n",
    "    Wfeatures_df = pd.merge(features_df, Wppg_df,left_on=['Season', 'TeamID_1'], right_on=['Season', 'TeamID'], how='left')\n",
    "    Wfeatures_df = Wfeatures_df.drop(['TeamID', 'GamesPlayed'], axis = 1)\n",
    "    features_df = pd.merge(Wfeatures_df, Lppg_df,left_on=['Season', 'TeamID_2'], right_on=['Season', 'TeamID'], how='left')\n",
    "    features_df = features_df.drop(['TeamID', 'GamesPlayed'], axis = 1)\n",
    "\n",
    "    Weff_df = eff_df.rename(columns = {'oEFF': 'oEFF_1', 'dEFF': 'dEFF_1','oEFF_30day': 'oEFF_30day_1', 'dEFF_30day': 'dEFF_30day_1'})\n",
    "    Leff_df = eff_df.rename(columns = {'oEFF': 'oEFF_2', 'dEFF': 'dEFF_2','oEFF_30day': 'oEFF_30day_2', 'dEFF_30day': 'dEFF_30day_2'})\n",
    "    features_df = pd.merge(features_df, Weff_df, how = 'left', left_on = ['Season', 'TeamID_1'], right_on = ['Season', 'TeamID'])\n",
    "    features_df = features_df.drop(['TeamID'], axis = 1)\n",
    "    features_df = pd.merge(features_df, Leff_df, how = 'left', left_on = ['Season', 'TeamID_2'], right_on = ['Season', 'TeamID'])\n",
    "    features_df = features_df.drop(['TeamID'], axis = 1)\n",
    "    features_df['PPG_diff'] = features_df['PPG_1'] - features_df['PPG_2']\n",
    "    features_df['WPerc_diff'] = features_df['WPerc_1'] - features_df['WPerc_2']\n",
    "    features_df['oEFF_diff'] = features_df['oEFF_1'] - features_df['oEFF_2']\n",
    "    features_df['dEFF_diff'] = features_df['dEFF_1'] - features_df['dEFF_2']\n",
    "    features_df['oEFF_30day_diff'] = features_df['oEFF_30day_1'] - features_df['oEFF_30day_2']\n",
    "    features_df['dEFF_30day_diff'] = features_df['dEFF_30day_1'] - features_df['dEFF_30day_2']\n",
    "    features_df = features_df.drop(['PPG_1', 'WPerc_1', 'PPG_2', 'WPerc_2', 'oEFF_1', 'dEFF_1', 'oEFF_30day_1', \n",
    "                                    'dEFF_30day_1', 'oEFF_2', 'dEFF_2', 'oEFF_30day_2', 'dEFF_30day_2'], axis = 1)\n",
    "    \n",
    "    Wrankings_df = rankings_df.rename(columns = {'MedianRank': 'MedianRank_1', 'MasseyRank': 'MasseyRank_1', 'MeanRank' : 'MeanRank_1'})\n",
    "    Lrankings_df = rankings_df.rename(columns = {'MedianRank': 'MedianRank_2', 'MasseyRank': 'MasseyRank_2', 'MeanRank' : 'MeanRank_2'})\n",
    "    features_df = pd.merge(features_df,Wrankings_df, how = 'left', left_on = ['Season', 'TeamID_1'], right_on = ['Season', 'TeamID'])\n",
    "    features_df = features_df.drop(['TeamID'],axis = 1)\n",
    "    \n",
    "    features_df = pd.merge(features_df,Lrankings_df, how = 'left', left_on = ['Season', 'TeamID_2'], right_on = ['Season', 'TeamID'])\n",
    "    \n",
    "    features_df = features_df.drop(['TeamID'],axis = 1)\n",
    "    features_df['MedianRank_diff'] = features_df['MedianRank_1'] - features_df['MedianRank_2']\n",
    "    features_df['MasseyRank_diff'] = features_df['MasseyRank_1'] - features_df['MasseyRank_2']\n",
    "    features_df['MeanRank_diff'] = features_df['MeanRank_1'] - features_df['MeanRank_2']\n",
    "    \n",
    "    features_df = features_df.drop(['MedianRank_1', 'MasseyRank_1', 'MeanRank_1',\n",
    "                                    'MedianRank_2', 'MasseyRank_2', 'MeanRank_2'], axis = 1)\n",
    "    \n",
    "    return features_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4beb47a",
   "metadata": {},
   "source": [
    "# The first part of the function merged the points-per-game dataframe, the efficiency metrics dataframe, and the rankings dataframe onto the respective test or training dataframe. This implies two sets of metrics in each row: one for Team 1 and another for Team 2. To reduce the number of features, but still maintain the same amount of information, we were able to take the difference between each metric for each team. So, for example, the column PPG_diff contains the difference in PPG between Team 1 and Team 2. This is done for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a4034",
   "metadata": {},
   "source": [
    "# Thus far, we have imported the data provided by the competition, and created functions for engineered features using this data. Now, we can move on to training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18d7d2",
   "metadata": {},
   "source": [
    "# Training & Testing The Model: 2015 To 2019 Regular Season\n",
    "After writing all of the data manipulation functions, we can run the imported data through each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d6136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare tournament submission df for 2015-2019, i.e. test set. \n",
    "## Note: not all matchups have Results, as we predict all possible matchups (11,390) for the 5 seasons when only 335 games actually occur\n",
    "features_df = prep_submission(ss_df,tcr_df)\n",
    "ppg_df = get_ppg(rscr_df)\n",
    "eff_df = get_efficiency(rsbs_df)\n",
    "rankings_df = get_rank(massey_df)\n",
    "tournament_df = merge_features(features_df, ppg_df,eff_df,rankings_df)\n",
    "\n",
    "## prepare regular season df for 2015-2019, i.e. training set\n",
    "rscr_df = prep_regseason(rscr_df)\n",
    "regseason_df = merge_features(rscr_df, ppg_df,eff_df,rankings_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b2d50",
   "metadata": {},
   "source": [
    "# Tournament_df contains our test data and regseason_df contains our training data.\n",
    "\n",
    "Next, I split each set into each season. This step is more a matter of preference and is not absolutely necessary. I prefer to have different model parameters for each season than to group them all together. This gives insight on how the evaluation metric can change from year to year,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258ff96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regseason_df_2015 = regseason_df[regseason_df.Season == 2015]\n",
    "regseason_df_2016 = regseason_df[regseason_df.Season == 2016]\n",
    "regseason_df_2017 = regseason_df[regseason_df.Season == 2017]\n",
    "regseason_df_2018 = regseason_df[regseason_df.Season == 2018]\n",
    "regseason_df_2019 = regseason_df[regseason_df.Season == 2019] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dfa040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop matchups that donâ€™t occur, as we cannot evaluate them\n",
    "tournament_df = tournament_df.dropna()\n",
    "\n",
    "tournament_df_2015 = tournament_df[tournament_df.Season == 2015]\n",
    "tournament_df_2016 = tournament_df[tournament_df.Season == 2016]\n",
    "tournament_df_2017 = tournament_df[tournament_df.Season == 2017]\n",
    "tournament_df_2018 = tournament_df[tournament_df.Season == 2018]\n",
    "tournament_df_2019 = tournament_df[tournament_df.Season == 2019] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698aa63",
   "metadata": {},
   "source": [
    "# Now we split the datasets into a test and training set. I only included the 2015 season below, but the code is identical for the other seasons (just swap the year in the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a31aefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = regseason_df_2015[['PPG_diff', 'WPerc_diff', 'oEFF_diff', 'dEFF_diff', 'oEFF_30day_diff',\n",
    "                       'dEFF_30day_diff', 'MedianRank_diff', 'MasseyRank_diff', 'MeanRank_diff']]\n",
    "y_train = regseason_df_2015[['Result']] \n",
    "X_test = tournament_df_2015[['PPG_diff', 'WPerc_diff', 'oEFF_diff', 'dEFF_diff', 'oEFF_30day_diff',\n",
    "                        'dEFF_30day_diff', 'MedianRank_diff', 'MasseyRank_diff', 'MeanRank_diff']]\n",
    "y_test = tournament_df_2015[['Result']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "184f046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5716522307172726"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                   solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "#train and predict\n",
    "clf.fit(X_train, np.ravel(y_train.values))\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "#log loss scoring \n",
    "log_loss(y_test, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16f0308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost as xgb\n",
    "\n",
    "#dtest = xgb.DMatrix(X_test, y_test, feature_names=X_test.columns)\n",
    "#dtrain = xgb.DMatrix(X_train, y_train,feature_names=X_train.columns)\n",
    "\n",
    "#param = {'verbosity':1, \n",
    "#         'objective':'binary:logistic',\n",
    "#         'booster':'gblinear',\n",
    "#         'eval_metric' :'logloss',\n",
    "#         'learning_rate': 0.05}\n",
    "\n",
    "#evallist = [(dtrain, 'train')] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066bed8",
   "metadata": {},
   "source": [
    "# Feel free to adjust the learning rate or any of the other parameters to see how they affect the final log loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6cdce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_round = 30\n",
    "# bst = xgb.train(param, dtrain, num_round, evallist) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
